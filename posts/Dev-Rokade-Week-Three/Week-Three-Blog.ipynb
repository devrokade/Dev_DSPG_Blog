{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Week Three Blog\"\n",
        "author: \"Dev Rokade\"\n",
        "date: \"2023-06-01\"\n",
        "categories: \"Week Three\"\n",
        "---"
      ],
      "id": "24e12198"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week Three Highlights\n",
        "\n",
        "We had a long weekend so Monday was a change than usual, and I got to sleep in!\n",
        "\n",
        "Tuesday and Wednesday- I was out of office due to health reasons\n",
        "\n",
        "Thursday- Worked on collecting data from Iowa Grocers excel file for prices on eggs, bacon and heirloom tomatoes. Finished cities in Iowa starting from the letter C and finished that. Proceeded to work on cities from letter I. Completed cities from O to R and W as well.\n",
        "\n",
        "Finished Datacamp Web Scraping Course.\n",
        "\n",
        "![](images/Screenshot%202023-06-01%20at%203.32.15%20PM.png)\n",
        "\n",
        "Friday- Started working on developing a webscraping script in Python using BeautifulSoup, requests and pandas.\n"
      ],
      "id": "d84829cc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "id": "fe648e29",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read the input Excel file\n",
        "input_file = \"grocery_websites.xlsx\"\n",
        "df = pd.read_excel(input_file)"
      ],
      "id": "ee956b8e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a new DataFrame to store the results\n",
        "result_df = pd.DataFrame(columns=[\"Website\", \"Product\", \"Price\"])"
      ],
      "id": "b2f050a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Scrape prices for each website\n",
        "for index, row in df.iterrows():\n",
        "    website = row[\"Website\"]\n",
        "    product = row[\"Product\"]\n",
        "    url = row[\"URL\"]\n",
        "\n",
        "try:\n",
        "        # Send a GET request to the website\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        \n",
        "        # Find the price element on the page\n",
        "        price_element = soup.find(\"span\", class_=\"price-amount\")\n",
        "        \n",
        "        if price_element:\n",
        "            price = price_element.text.strip()\n",
        "            result_df = result_df.append({\"Website\": website, \"Product\": product, \"Price\": price}, ignore_index=True)\n",
        "        else:\n",
        "            result_df = result_df.append({\"Website\": website, \"Product\": product, \"Price\": \"Not found\"}, ignore_index=True)\n",
        "    \n",
        "except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error scraping {website}: {e}\")\n",
        "        result_df = result_df.append({\"Website\": website, \"Product\": product, \"Price\": \"Error\"}, ignore_index=True)\n"
      ],
      "id": "1321da02",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write the results to a new Excel file\n",
        "output_file = \"grocery_prices.xlsx\"\n",
        "result_df.to_excel(output_file, index=False)"
      ],
      "id": "382a2b07",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is still in its working phase so hasn't been finished yet, but working on it so that web scraping gets as autonomous as possible."
      ],
      "id": "ef8cf85c"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}