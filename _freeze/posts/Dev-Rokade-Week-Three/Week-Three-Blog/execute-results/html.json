{
  "hash": "4c5e707aef540c8e3f14eaa8c5877461",
  "result": {
    "markdown": "---\ntitle: \"Week Three Blog\"\nauthor: \"Dev Rokade\"\ndate: \"2023-06-01\"\ncategories: \"Week Three\"\n---\n\n# Week Three Highlights\n\nWe had a long weekend so Monday was a change than usual, and I got to sleep in!\n\nTuesday and Wednesday- I was out of office due to health reasons\n\nThursday- Worked on collecting data from Iowa Grocers excel file for prices on eggs, bacon and heirloom tomatoes. Finished cities in Iowa starting from the letter C and finished that. Proceeded to work on cities from letter I. Completed cities from O to R and W as well.\n\nFinished Datacamp Web Scraping Course.\n\n![](images/Screenshot%202023-06-01%20at%203.32.15%20PM.png)\n\nFriday- Started working on developing a webscraping script in Python using BeautifulSoup, requests and pandas.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Read the input Excel file\ninput_file = \"grocery_websites.xlsx\"\ndf = pd.read_excel(input_file)\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Create a new DataFrame to store the results\nresult_df = pd.DataFrame(columns=[\"Website\", \"Product\", \"Price\"])\n```\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Scrape prices for each website\nfor index, row in df.iterrows():\n    website = row[\"Website\"]\n    product = row[\"Product\"]\n    url = row[\"URL\"]\n\ntry:\n        # Send a GET request to the website\n        response = requests.get(url)\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        \n        # Find the price element on the page\n        price_element = soup.find(\"span\", class_=\"price-amount\")\n        \n        if price_element:\n            price = price_element.text.strip()\n            result_df = result_df.append({\"Website\": website, \"Product\": product, \"Price\": price}, ignore_index=True)\n        else:\n            result_df = result_df.append({\"Website\": website, \"Product\": product, \"Price\": \"Not found\"}, ignore_index=True)\n    \nexcept requests.exceptions.RequestException as e:\n        print(f\"Error scraping {website}: {e}\")\n        result_df = result_df.append({\"Website\": website, \"Product\": product, \"Price\": \"Error\"}, ignore_index=True)\n\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Write the results to a new Excel file\noutput_file = \"grocery_prices.xlsx\"\nresult_df.to_excel(output_file, index=False)\n```\n:::\n\n\nThis is still in its working phase so hasn't been finished yet, but working on it so that web scraping gets as autonomous as possible.\n\n",
    "supporting": [
      "Week-Three-Blog_files"
    ],
    "filters": [],
    "includes": {}
  }
}